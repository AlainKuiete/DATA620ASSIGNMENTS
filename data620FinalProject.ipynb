{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nxviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6d4246fe6836>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnxviz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatrixPlot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCircosPlot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nxviz'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from nxviz import MatrixPlot, CircosPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4881373ea2c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:\\\\DATA620\\\\project3_dataset\\\\edit-enwikinews.tar\\\\edit-enwikinews\\\\edit-enwikinews\\\\out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:\\\\DATA620\\\\project3_dataset\\\\edit-enwikinews.tar\\\\edit-enwikinews\\\\edit-enwikinews\\\\out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = 1545730073\n",
    "dt_object = datetime.fromtimestamp(timestamp)\n",
    "\n",
    "print(\"dt_object =\", dt_object)\n",
    "print(\"type(dt_object) =\", type(dt_object))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a graph from the pandas DataFrame\n",
    "Let's start by creating a graph from a pandas DataFrame. In this exercise, you'll create a new bipartite graph by looping over the edgelist (which is a DataFrame object).\n",
    "\n",
    "For simplicity's sake, in this graph construction procedure, any edge between a student and a forum node will be the 'last' edge (in time) that a student posted to a forum over the entire time span of the dataset, though there are ways to get around this.\n",
    "\n",
    "Additionally, to shorten the runtime of the exercise, we have provided a sub-sampled version of the edge list as data. Explore it in the IPython Shell to familiarize yourself with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new Graph: G\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes from each of the partitions\n",
    "G.add_nodes_from(data['student'], bipartite = 'student')\n",
    "G.add_nodes_from(data['forum'], bipartite = 'forum')\n",
    "\n",
    "# Add in each edge along with the date the edge was created\n",
    "for r, d in data.iterrows():\n",
    "    G .add_edge(d['student'], d['forum'], date=d['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the degree centrality score of each node to their metadata dictionary\n",
    "dcs = nx.degree_centrality(G)\n",
    "for n in G.nodes():\n",
    "    G.nodes[n]['centrality'] = dcs[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis\n",
    "Here, a graph G has been loaded into your workspace. Your task is to perform some exploratory analysis in the IPython Shell. What is the type of this graph? How many nodes and edges are present?\n",
    "\n",
    "The .nodes() and .edges() methods will be useful to you here, as will the type() and len() functions. Using these, you can start describing the graph's basic statistics. After you have explored the graph, choose the right option below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are: {} nodes'.format(len(G.nodes(data=True))))\n",
    "print('There are: {} edges'.format(len(G.edges(data=True))))\n",
    "print('The graph type is: ', type(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting using nxviz\n",
    "Now, you're going to practice creating a CircosPlot using nxviz! As a bonus preview of what's coming up in the next video, there's a little segment on the bipartite keyword in this exercise!\n",
    "\n",
    "Here, the degree centrality score of each node has been added to their metadata dictionary for you using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CircosPlot object: c\n",
    "c = CircosPlot(graph = G, node_color = 'bipartite', node_grouping = 'bipartite', node_order = 'centrality')\n",
    "\n",
    "# Draw c to the screen\n",
    "c.draw()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartite graphs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The bipartite keyword\n",
    "In the video, Eric introduced you to the 'bipartite' keyword. This keyword is part of a node's metadata dictionary, and can be assigned both when you add a node and after the node is added. Remember, though, that by definition, in a bipartite graph, a node cannot be connected to another node in the same partition.\n",
    "\n",
    "Here, you're going to write a function that returns the nodes from a given partition in a bipartite graph. In this case, the relevant partitions of the Github bipartite graph you'll be working with are 'projects' and 'users'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get_nodes_from_partition()\n",
    "def get_nodes_from_partition(G, partition):\n",
    "    # Initialize an empty list for nodes to be returned\n",
    "    nodes = []\n",
    "    # Iterate over each node in the graph G\n",
    "    for n in G.nodes():\n",
    "        # Check that the node belongs to the particular partition\n",
    "        if G.node[n]['bipartite'] == partition:\n",
    "            # If so, append it to the list of nodes\n",
    "            nodes.append(n)\n",
    "    return nodes\n",
    "\n",
    "# Print the number of nodes in the 'projects' partition\n",
    "print(len(get_nodes_from_partition(G, 'projects')))\n",
    "\n",
    "# Print the number of nodes in the 'users' partition\n",
    "print(len(get_nodes_from_partition(G, 'users')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree centrality distribution of user nodes\n",
    "In this exercise and the next one, you're going to do a final recap of material from the previous course. Your task is to plot the degree centrality distributions for each node partition in the bipartite version of the GitHub collaboration network. Here, you'll do this for the 'users' partition. In the next exercise, you'll do this for the 'projects' partition.\n",
    "\n",
    "The function you wrote before, get_nodes_from_partition(), has been loaded for you. Just to remind you, the \"degree centrality\" is a measure of node importance, and the \"degree centrality distribution\" is the list of degree centrality scores for all nodes in the graph. A few exercises ago, when you made the circos plot, we computed the degree centralities for you. You'll now practice doing this yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the 'users' nodes: user_nodes\n",
    "user_nodes = get_nodes_from_partition(G, 'users')\n",
    "\n",
    "# Compute the degree centralities: dcs\n",
    "dcs = nx.degree_centrality(G)\n",
    "\n",
    "# Get the degree centralities for user_nodes: user_dcs\n",
    "user_dcs = [dcs[n] for n in user_nodes]\n",
    "\n",
    "# Plot the degree distribution of users_dcs\n",
    "plt.yscale('log')\n",
    "plt.hist(user_dcs, bins=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree centrality distribution of project nodes\n",
    "Now it's time to plot the degree centrality distribution for the 'projects' partition of G. The steps to do this are exactly the same as in the previous exercise. For your convenience, matplotlib.pyplot has been pre-imported as plt.\n",
    "\n",
    "Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 'projects' nodes: project_nodes\n",
    "project_nodes = get_nodes_from_partition(G, 'projects')\n",
    "\n",
    "# Compute the degree centralities: dcs\n",
    "dcs = nx.degree_centrality(G)\n",
    "\n",
    "# Get the degree centralities for project_nodes: project_dcs\n",
    "project_dcs = [dcs[n] for n in project_nodes]\n",
    "\n",
    "# Plot the degree distribution of project_dcs\n",
    "plt.yscale('log')\n",
    "plt.hist(project_dcs, bins=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartite graphs and recommendation systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shared nodes in other partition\n",
    "In order to build up your concept of recommendation systems, we are going to start with the fundamentals. The focus here is on computing user similarity in bipartite graphs.\n",
    "\n",
    "Your job is to write a function that takes in two nodes, and returns the set of repository nodes that are shared between the two user nodes.\n",
    "\n",
    "You'll find the following methods and functions helpful in this exercise - .neighbors(), set(), and .intersection() - besides, of course, the shared_partition_nodes function that you will define!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_partition_nodes(G, node1, node2):\n",
    "    # Check that the nodes belong to the same partition\n",
    "    assert G.node[node1]['bipartite'] == G.node[node2]['bipartite']\n",
    "\n",
    "    # Get neighbors of node 1: nbrs1\n",
    "    nbrs1 = G.neighbors(node1)\n",
    "    # Get neighbors of node 2: nbrs2\n",
    "    nbrs2 = G.neighbors(node2)\n",
    "\n",
    "    # Compute the overlap using set intersections\n",
    "    overlap = set(nbrs1).intersection(nbrs2)\n",
    "    return overlap\n",
    "\n",
    "# Print the number of shared repositories between users 'u7909' and 'u2148'\n",
    "print(len(shared_partition_nodes(G, 'u7909', 'u2148')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User similarity metric\n",
    "Having written a function to calculate the set of nodes that are shared between two nodes, you're now going to write a function to compute a metric of similarity between two users: the number of projects shared between two users divided by the total number of nodes in the other partition. This can then be used to find users that are similar to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_similarity(G, user1, user2, proj_nodes):\n",
    "    # Check that the nodes belong to the 'users' partition\n",
    "    assert G.node[user1]['bipartite'] == 'users'\n",
    "    assert G.node[user2]['bipartite'] == 'users'\n",
    "\n",
    "    # Get the set of nodes shared between the two users\n",
    "    shared_nodes = shared_partition_nodes(G, user1, user2)\n",
    "\n",
    "    # Return the fraction of nodes in the projects partition\n",
    "    return len(shared_nodes) / len(G.nodes(proj_nodes))\n",
    "\n",
    "# Compute the similarity score between users 'u4560' and 'u1880'\n",
    "project_nodes = get_nodes_from_partition(G, 'projects')\n",
    "similarity_score = user_similarity(G, 'u4560', 'u1880', 'projects')\n",
    "\n",
    "print(similarity_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find similar users\n",
    "You're now going to build upon what you've learned so far to write a function called most_similar_users() that finds the users most similar to another given user.\n",
    "\n",
    "The beginnings of this function have been written for you. A list of nodes, user_nodes has been created, which contains all of the users except the given user that has been passed into the function. Your task is to complete the function such that it finds the users most similar to this given user. You'll make use of your user_similarity() function from the previous exercise to help do this.\n",
    "\n",
    "A dictionary called similarities has been setup, in which the keys are the scores and the list of values are the nodes. If you've never seen a defaultdict before, don't worry - you'll learn more about it in Chapter 3! It functions exactly like a regular Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def most_similar_users(G, user, user_nodes, proj_nodes):\n",
    "    # Data checks\n",
    "    assert G.node[user]['bipartite'] == 'users'\n",
    "\n",
    "    # Get other nodes from user partition\n",
    "    user_nodes = set(user_nodes)\n",
    "    user_nodes.remove(user)\n",
    "\n",
    "    # Create the dictionary: similarities\n",
    "    similarities = defaultdict(list)\n",
    "    for n in user_nodes:\n",
    "        similarity = user_similarity(G, user, n, proj_nodes)\n",
    "        similarities[similarity].append(n)\n",
    "\n",
    "    # Compute maximum similarity score: max_similarity\n",
    "    max_similarity = max(similarities.keys())\n",
    "\n",
    "    # Return list of users that share maximal similarity\n",
    "    return similarities[max_similarity]\n",
    "\n",
    "user_nodes = get_nodes_from_partition(G, 'users')\n",
    "project_nodes = get_nodes_from_partition(G, 'projects')\n",
    "\n",
    "print(most_similar_users(G, 'u4560', user_nodes, project_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommend repositories\n",
    "You're close to the end! Here, the task is to practice using set differences, and you'll apply it to recommending repositories from a second user that the first user should contribute to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_repositories(G, from_user, to_user):\n",
    "    # Get the set of repositories that from_user has contributed to\n",
    "    from_repos = set(G.neighbors(from_user))\n",
    "    # Get the set of repositories that to_user has contributed to\n",
    "    to_repos = set(G.neighbors(to_user))\n",
    "\n",
    "    # Identify repositories that the from_user is connected to that the to_user is not connected to\n",
    "    return from_repos.difference(to_repos)\n",
    "\n",
    "# Print the repositories to be recommended\n",
    "print(recommend_repositories(G, 'u7909', 'u2148'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept of projection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading graphs\n",
    "In this exercise, before you compute projections, you're going to practice working with one of NetworkX's disk I/O functions, read_edgelist(). read_edgelist() creates a graph from the edgelist file. The graph that you'll be working with is a bipartite graph describing the American Revolution. There are two node partitions - 'people' and 'clubs', and edges denote a person being a member of a club."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Read in the data: g\n",
    "G = nx.read_edgelist('american-revolution.edgelist')\n",
    "# Assign nodes to 'clubs' or 'people' partitions\n",
    "for n, d in G.nodes(data=True):\n",
    "    if '.' in n:\n",
    "        G.nodes[n]['bipartite'] = 'people'\n",
    "    else:\n",
    "        G.nodes[n]['bipartite'] = 'clubs'\n",
    "        \n",
    "# Print the edges of the graph\n",
    "print(G.edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing projection\n",
    "It's now time to try your hand at computing the projection of a bipartite graph to the nodes on one of its partitions. This will help you gain practice with converting between a bipartite version of a graph and its unipartite projections. Remember from the video that the \"projection\" of a graph onto one of its partitions is the connectivity of the nodes in that partition conditioned on connections to nodes on the other partition. Made more concretely, you can think of the \"connectivity of customers based on shared purchases\".\n",
    "\n",
    "To help you get started, here's a hint on list comprehensions. List comprehensions can include conditions, so if you want to filter a graph for a certain type of node, you can do: [n for n, d in G.nodes(data=True) if d['key'] == 'some_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the nodelists needed for computing projections: people, clubs\n",
    "# This exercise shows you two ways to do it, one with `data=True` and one without.\n",
    "people = [n for n in G.nodes() if G.nodes[n]['bipartite'] == 'people']\n",
    "clubs = [n for n, d in G.nodes(data=True) if d['bipartite'] == 'clubs']\n",
    "\n",
    "# Compute the people and clubs projections: peopleG, clubsG\n",
    "peopleG = nx.bipartite.projected_graph(G,people)\n",
    "clubsG = nx.bipartite.projected_graph(G, clubs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot degree centrality on projection\n",
    "Here, you're going to compare the degree centrality distributions for each of the following graphs: the original graph G, the people graph projection peopleG, and the clubs graph projection clubsG. This will reinforce the difference in degree centrality score computation between bipartite and unipartite versions of degree centrality metrics. The node lists people and clubs have been pre-loaded for you.\n",
    "\n",
    "Recall from the video that the bipartite functions require passing in a container of nodes, but will return all degree centrality scores nonetheless. Remember also that degree centrality scores are stored as dictionaries (mapping node to score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Plot the degree centrality distribution of both node partitions from the original graph\n",
    "plt.figure()\n",
    "original_dc = nx.bipartite.degree_centrality(G,people)\n",
    "# Remember that you can directly plot dictionary values.\n",
    "plt.hist(original_dc.values(), alpha=0.5)\n",
    "plt.yscale('log')\n",
    "plt.title('Bipartite degree centrality')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the degree centrality distribution of the peopleG graph\n",
    "plt.figure()  \n",
    "people_dc = nx.degree_centrality(peopleG)\n",
    "plt.hist(people_dc.values())\n",
    "plt.yscale('log')\n",
    "plt.title('Degree centrality of people partition')\n",
    "plt.show()\n",
    "\n",
    "# Plot the degree centrality distribution of the clubsG graph\n",
    "plt.figure() \n",
    "clubs_dc = nx.degree_centrality(clubsG)\n",
    "plt.hist(clubs_dc.values())\n",
    "plt.yscale('log')\n",
    "plt.title('Degree centrality of clubs partition')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartite graphs as matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute adjacency matrix\n",
    "Now, you'll get some practice using matrices and sparse matrix multiplication to compute projections! In this exercise, you'll use the matrix multiplication operator @ that was introduced in Python 3.5.\n",
    "\n",
    "You'll continue working with the American Revolution graph. The two partitions of interest here are 'people' and 'clubs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of people and list of clubs from the graph: people_nodes, clubs_nodes\n",
    "people_nodes = get_nodes_from_partition(G,'people')\n",
    "clubs_nodes = get_nodes_from_partition(G, 'clubs')\n",
    "\n",
    "# Compute the biadjacency matrix: bi_matrix\n",
    "bi_matrix = nx.bipartite.biadjacency_matrix(G, row_order=people_nodes, column_order=clubs_nodes)\n",
    "\n",
    "# Compute the user-user projection: user_matrix\n",
    "user_matrix = bi_matrix @ bi_matrix.T\n",
    "\n",
    "print(user_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find shared membership: Transposition\n",
    "As you may have observed, you lose the metadata from a graph when you go to a sparse matrix representation. You're now going to learn how to impute the metadata back so that you can learn more about shared membership.\n",
    "\n",
    "The user_matrix you computed in the previous exercise has been preloaded into your workspace.\n",
    "\n",
    "Here, the np.where() function will prove useful. This is what it does: given an array, say, a = [1, 5, 9, 5], if you want to get the indices where the value is equal to 5, you can use idxs = np.where(a == 5). This gives you back an array in a tuple, (array([1, 3]),). To access those indices, you would want to index into the tuple as idxs[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the names of people who were members of the most number of clubs\n",
    "diag = user_matrix.diagonal() \n",
    "indices = np.where(diag == diag.max())[0]  \n",
    "print('Number of clubs: {0}'.format(diag.max()))\n",
    "print('People with the most number of memberships:')\n",
    "for i in indices:\n",
    "    print('- {0}'.format(people_nodes[i]))\n",
    "\n",
    "# Set the diagonal to zero and convert it to a coordinate matrix format\n",
    "user_matrix.setdiag(0)\n",
    "users_coo = user_matrix.tocoo()\n",
    "\n",
    "# Find pairs of users who shared membership in the most number of clubs\n",
    "indices2 = np.where(users_coo.data == users_coo.data.max())[0]\n",
    "print('People with most number of shared memberships:')\n",
    "for idx in indices2:\n",
    "    print('- {0}, {1}'.format(people_nodes[users_coo.row[idx]], people_nodes[users_coo.col[idx]]))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing network data with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make nodelist\n",
    "You're now going to practice converting graphs to pandas representation. If you have taken any of DataCamp's pandas courses, you will know that there is a DataFrame.to_csv('filename.csv') method that lets you save it as a CSV file, which is a human-readable version. The main concept we hope you take away from here is the process of converting a graph to a list of records.\n",
    "\n",
    "Start by re-familiarizing yourself with the graph data structure by calling G.nodes(data=True)[0] in the IPython Shell to examine one node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store each edge as a record: nodelist\n",
    "nodelist = []\n",
    "for n, d in G_people.nodes(data=True):\n",
    "    # nodeinfo stores one \"record\" of data as a dict\n",
    "    nodeinfo = {'person': n} \n",
    "    \n",
    "    # Update the nodeinfo dictionary \n",
    "    nodeinfo.update(d)\n",
    "    \n",
    "    # Append the nodeinfo to the node list\n",
    "    nodelist.append(nodeinfo)\n",
    "    \n",
    "\n",
    "# Create a pandas DataFrame of the nodelist: node_df\n",
    "node_df = pd.DataFrame(nodelist)\n",
    "print(node_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make edgelist\n",
    "Now, you're going to apply the same ideas to making an edge list. Go forth and give it a shot!\n",
    "\n",
    "As with the previous exercise, run G.edges(data=True)[0] in the IPython Shell to get a feel for the edge list data structure before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store each edge as a record: edgelist\n",
    "edgelist = []\n",
    "for n1, n2, d in G_people.edges(data=True):\n",
    "    # Initialize a dictionary that shows edge information: edgeinfo\n",
    "    edgeinfo = {'node1': n1, 'node2': n2}\n",
    "    \n",
    "    # Update the edgeinfo data with the edge metadata\n",
    "    edgeinfo.update(d)\n",
    "    \n",
    "    # Append the edgeinfo to the edgelist\n",
    "    edgelist.append(edgeinfo)\n",
    "    \n",
    "# Create a pandas DataFrame of the edgelist: edge_df\n",
    "edge_df = pd.DataFrame(edgelist)\n",
    "print(edge_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to graph differences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of graphs\n",
    "In this set of exercises, you'll use a college messaging dataset to learn how to filter graphs for time series analysis. In this dataset, nodes are students, and edges denote messages being sent from one student to another. The graph as it stands right now captures all communications at all time points.\n",
    "\n",
    "Let's start by analyzing the graphs in which only the edges change over time.\n",
    "\n",
    "The dataset has been loaded into a DataFrame called data. Feel free to explore it in the IPython Shell. Specifically, check out the output of data['sender'] and data['recipient']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = range(4, 11)\n",
    "\n",
    "# Initialize an empty list: Gs\n",
    "Gs = [] \n",
    "for month in months:\n",
    "    # Instantiate a new undirected graph: G\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add in all nodes that have ever shown up to the graph\n",
    "    G.add_nodes_from(data['sender'])\n",
    "    G.add_nodes_from(data['recipient'])\n",
    "    \n",
    "    # Filter the DataFrame so that there's only the given month\n",
    "    df_filtered = data[data['month'] == month]\n",
    "    \n",
    "    # Add edges from filtered DataFrame\n",
    "    G.add_edges_from(zip(df_filtered['sender'], df_filtered['recipient']))\n",
    "    \n",
    "    # Append G to the list of graphs\n",
    "    Gs.append(G)\n",
    "    \n",
    "print(len(Gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph differences over time\n",
    "Now, you'll compute the graph differences over time! To look at the simplest case, here you'll use a window of (month, month + 1), and then keep track of the edges gained or lost over time. This exercise is preparation for the next exercise, in which you will visualize the changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Instantiate a list of graphs that show edges added: added\n",
    "added = []\n",
    "# Instantiate a list of graphs that show edges removed: removed\n",
    "removed = []\n",
    "# Here's the fractional change over time\n",
    "fractional_changes = []\n",
    "window = 1  \n",
    "i = 0      \n",
    "\n",
    "for i in range(len(Gs) - window):\n",
    "    g1 = Gs[i]\n",
    "    g2 = Gs[i + window]\n",
    "        \n",
    "    # Compute graph difference here\n",
    "    added.append(nx.difference(g2, g1))   \n",
    "    removed.append(nx.difference(g1, g2))\n",
    "    \n",
    "    # Compute change in graph size over time\n",
    "    fractional_changes.append((len(g2.edges()) - len(g1.edges())) / len(g1.edges()))\n",
    "    \n",
    "# Print the fractional change\n",
    "print(fractional_changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot number of edge changes over time\n",
    "You're now going to make some plots! All of the lists that you've created before have been loaded for you in this exercise too. Do not worry about some of the fancy matplotlib code that shows up below: there are comments to help you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "# Plot the number of edges added over time\n",
    "edges_added = [len(g.edges()) for g in added]\n",
    "plot1 = ax1.plot(edges_added, label='added', color='orange')\n",
    "\n",
    "# Plot the number of edges removed over time\n",
    "edges_removed = [len(g.edges()) for g in removed]\n",
    "plot2 = ax1.plot(edges_removed, label='removed', color='purple')\n",
    "\n",
    "# Set yscale to logarithmic scale\n",
    "ax1.set_yscale('log')  \n",
    "ax1.legend()\n",
    "\n",
    "# 2nd axes shares x-axis with 1st axes object\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the fractional changes over time\n",
    "plot3 = ax2.plot(fractional_changes, label='fractional change', color='green')\n",
    "\n",
    "# Here, we create a single legend for both plots\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc=0)\n",
    "plt.axhline(0, color='green', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolving graph statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of edges over time\n",
    "You're now going to get some practice plotting other evolving graph statistics. We'll start with a simpler exercise to kick things off. First off, plot the number of edges over time.\n",
    "\n",
    "To do this, you'll create a list of the number of edges per month. The index of this list will correspond to the months elapsed since the first month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# Create a list of the number of edges per month\n",
    "edge_sizes = [len(g.edges()) for g in Gs]\n",
    "\n",
    "# Plot edge sizes over time\n",
    "plt.plot(edge_sizes)\n",
    "plt.xlabel('Time elapsed from first month (in months).') \n",
    "plt.ylabel('Number of edges')                           \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree centrality over time\n",
    "Now, you're going to plot the degree centrality distribution over time. Remember that the ECDF function will be provided, so you won't have to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a list of degree centrality scores month-by-month\n",
    "cents = []\n",
    "for G in Gs:\n",
    "    cent = nx.degree_centrality(G)\n",
    "    cents.append(cent)\n",
    "\n",
    "\n",
    "# Plot ECDFs over time\n",
    "fig = plt.figure()\n",
    "for i in range(len(cents)):\n",
    "    x, y = ECDF(cents[i].values()) \n",
    "    plt.plot(x, y, label='Month {0}'.format(i+1)) \n",
    "plt.legend()   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortest Path Between Nodes Breath-Fist Search BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_exists(G, node1, node2):\n",
    "    \"\"\"\n",
    "    This function checks whether a path exists between two nodes (node1, node2) in graph G.\n",
    "    \"\"\"\n",
    "    visited_nodes = set()\n",
    "    queue = [node1]\n",
    "\n",
    "    for node in queue:\n",
    "        neighbors = G.neighbors(node)\n",
    "        if node2 in neighbors:\n",
    "            print('Path exists between nodes {0} and {1}'.format(node1, node2))\n",
    "            return True\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            visited_nodes.add(node)\n",
    "            queue.extend([n for n in neighbors if n not in visited_nodes])\n",
    "\n",
    "        # Check to see if the final element of the queue has been reached\n",
    "        if node == queue[-1]:\n",
    "            print('Path does not exist between nodes {0} and {1}'.format(node1, node2))\n",
    "\n",
    "            # Place the appropriate return statement\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming in & zooming out: Overall graph summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find nodes with top degree centralities\n",
    "In this exercise, you'll take a deeper dive to see whether there's anything interesting about the most connected students in the network. First off, you'll find the cluster of students that have the highest degree centralities. This result will be saved for the next plotting exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 5 unique degree centrality scores: top_dcs\n",
    "top_dcs = sorted(set(nx.degree_centrality(G).values()), reverse=True)[0:5]\n",
    "\n",
    "# Create list of nodes that have the top 5 highest overall degree centralities\n",
    "top_connected = []\n",
    "for n, dc in nx.degree_centrality(G).items():\n",
    "    if dc in top_dcs:\n",
    "        top_connected.append(n)\n",
    "        \n",
    "# Print the number of nodes that share the top 5 degree centrality scores\n",
    "print(len(top_connected))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing connectivity\n",
    "Here, you're going to visualize how the connectivity of the top connected nodes changes over time. The list of top connected values, top_connected, from the previous exercise has been loaded.\n",
    "\n",
    "Remember the defaultdict you used in Chapter 1? You'll use another defaultdict in this exercise! As Eric mentioned in the video, a defaultdict is preferred here as a regular Python dictionary would throw a KeyError if you try to get an item with a key that is not currently in the dictionary.\n",
    "\n",
    "This exercise will make use of nested for loops. That is, you'll use one for loop inside another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a defaultdict in which the keys are nodes and the values are a list of connectivity scores over time\n",
    "connectivity = defaultdict(list)\n",
    "for n in top_connected:\n",
    "    for g in Gs:\n",
    "        connectivity[n].append(len(list(g.neighbors(n))))\n",
    "\n",
    "# Plot the connectivity for each node\n",
    "fig = plt.figure() \n",
    "for n, conn in connectivity.items(): \n",
    "    plt.plot(conn, label=n) \n",
    "plt.legend()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
